
=========================
    EBE-VISNNU Readme
=========================

The EBE-VISNNU package is a convenience package to automate event-by-event hybrid calculations. It divide calculatons into "jobs", where each job consists multiple "ebe-calculations". Each "ebe-calculation" is a complete hybrid calculation that in execution order performs: heavy-ion event generation (superMC), hydrodynamics simulation (VISHNew), particle emission sampling (iSS), hadron rescattering simulation (osc2u and urqmd), flow calculation (binUtilities), and finally collect important results to databases (EbeCollector). Each "job" runs the given number of "ebe-calculations" sequentially, and "jobs" are run in parallel. The package has utility scripts that can combine the generated SQLite database files from different jobs into one that can be analyzed later.

The main programs are contained in the subfolder "EBE-Node", which is used to perform one job, and which will be duplicated when multiple jobs are desired. The package needs two locations to perform multi-job calculations: one folder is used to store duplications of "EBE-Node" and intermediate results generated during the calculation (refer to as "tmp folder" in the following), and another folder is used to store final results (refer to as "result folder" in the following). By default the tmp folder is named as "PlayGround" and the result folder is named as "RESULTS", both in the root directory of the package.

------------------------------------------------------------------
<<1>> How to use the package to perform multi-job calculations
------------------------------------------------------------------

This section explains how to use the highest-level scripts provided by the package to perform event-by-event hybrid calculations. This section should be the only section requires reading for user who are not interested in modifying the package.

*VERY IMPORTANT* Make sure you have Python 2.7+ (or Python 3) installed before proceeding.

In the following all path are relative to the root directory of the package.

Step 1) Check requirements.

It is always a good idea to first check if all the required libraries are installed on the system. To do this run the checkEnviorment.py script from /utilities/:
$ ./checkEnvironment.py

You should see a list of packages that are either installed or not installed. If you do not see the sentence "All essential packages installed.", please install the missing packages.

Step 2) Compile executables.

The package actually has the power to compile the required binary on-the-fly if it is found missing. However this compiling is done individually for each job and if the number of jobs is huge this is a great waste of resources. It is recommended to pre-compile all the programs. To do so use the script compile-all.sh from /EBE-Node/crank:
$ ./compile-all.sh

You should see a bunch of output generated by various compilers. Just simply igore them and wait for the script to say that "Compiling finished.".

Step 3) Generate jobs.

To generate jobs use the generateJobs.py script in the root directory. Most of the runnable scripts in this package provide the feature that if you run it without additional arguments it will print the usage echo, for example:
$ ./generateJobs.py
And you should see the output:
Usage: generateJobs.py number_of_jobs number_of_events_per_job [results_folder="./RESULTS"] [walltime="03:00:00"] [working_folder="./PlayGround"] [compress_results_folder="yes"]

The echo says that the 1st argument for the script should specify the number of jobs you want to generate; the 2nd argument specifying the number of ebe-calculations for each job; the 3rd argument points to the result folder; the 4th argument specifies the "wall time" (used in torque system, explain later); the 5th argument points to the tmp folder; and the 6th argument is for whether to compress final results. Except for the first two, all other arguments have default values. The simplest way to generate jobs is just to accept the default values, and as an example to generate 2 jobs, eaching performing 5 ebe-calculations, simply do the following:
$ ./generateJobs.py 2 5

After you see the echo "Jobs generated.", you should see the tmp folder "PlayGround" and the result folder "RESULTS" in the root directory if not existing previously.

Step 4) Submit jobs.

The way to submit jobs depends on the system. For a cluster that has "torque" scheduling system (therefore the "qsub" command is availible), submit jobs use the submitJobs_qsub.py script; for a local computation use the submitJobs_local.py script. The difference is that the local computation is only parallelled for the local CPUs and calculation on cluster, via the torque system, will be distributed to multiple nodes. Both scripts accept one argument specifying the location of the tmp folder, which is default to "PlayGround", so to submit a local calculation simply do:
$ ./submitJobs_local.py

You should see some feedbacks listing the jobs that have been submitted.

Step 5) Checking progress.

Progresses for each job can be checked by the progressReport.py script in the root directory:
$ ./progressReport.py

It will list the current progress for all jobs.

Step 6) Combining databases.

The most import results from each job will be automatically collected into SQLite databases upon finishing. These databases, together with other useful intermediate results are all stored in the result folder, which defaults to "RESULTS". To combined the database files from different jobs into one use the combineEbeDatabasesFromZippedResults.py script under /utilities/, which accepts one argument pointing to the result folder, for example:
$ ./combineEbeDatabasesFromZippedResults.py ../RESULTS/

You should see a new file "collected.db" appeared in the /RESULTS/ folder. This is the all-in-one database files that should be used for further analyses.

Step 7) Analyze data.

The "collected.db" generated from previous steps is the SQLite database file that can be analyzed by any desired means. One can use the utility "sqlite3" comes with the SQLite package to analyze it; one can use a script to read it into Matlab and analyze it (preferred way); or one can use the ./unpackDatabase.py script located under /EBE-Node/EbeCollector/ to dump the whole database into separated space-separated text files, each for individual table. For example, running the following under /RESULTS/:
$ ../EBE-Node/EbeCollector/unpackDatabase.py ./collected.db . "#"
will generate several ".dat" files, each containing data for the corresponding type. Each file have a one-line header to indicate what data each column records, and the rest are data separated by spaces.

For details about the structure of the database see /EBE-Node/EbeCollector/EbeCollector_readme.txt.

Step n) Tuning parameters.

After you familiarized yourself about how to perform multi-job hybrid calculations, finally the time comes to the question about how to tune parameters for the simulations. The most commonly tuned parameters are in the ParameterDict.py file in the directory, which should be the only file use to direct the simluations. This file will be copied to the result folder for record-keeping purpose when generating jobs.

----------------------------------
<<2>> Structure of the package
----------------------------------

This section give more details on the structure of the package. It is not needed for using the package; only users who want to modify the package should read this section.

TO-FINISH

------------------------------------------------
<<3>> Running time and efficiency discussion
------------------------------------------------

TO-FINISH